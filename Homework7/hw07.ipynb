{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Apache Spark for complex queries\n",
    "\n",
    "## Data Management Homework 7\n",
    "\n",
    "In this assignment we will use \n",
    "[Apache Spark](https://spark.apache.org/): \n",
    "a popular framework for optimal distributed processing on large amount of data. \n",
    "The objective of is to use Apache Spark to translate and execute some queries of the TPCx-BB bigdata benchmark.  \n",
    "[TPCx-BB](https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) \n",
    "or simply \"Big Bench\" is a common benchmark suite to evaluate the system performance on big data analytics and machine learning algorithms. \n",
    "We will focus on big data analytical queries, which are expressed in SQL. \n",
    "\n",
    "Spark is a framework available in multiple languages: Scala, Java, Python, R. In this exercise, we will use Python.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Jupyter Lab\n",
    "\n",
    "If you are not familiar with the Jupyter Lab environment, check out these resources from the official website: \n",
    "[example notebook](https://jupyter.org/try), \n",
    "[docs](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html). \n",
    "\n",
    "Quick reference:\n",
    "- This is a cell. A cell can contain either Markdown text (such as this one) or code. Everything in jupyter notebook is a cell.\n",
    "    - Click on the plus on the top bar to add a new cell\n",
    "    - You can double-click on a text cell to edit iy using Markdown\n",
    "    - You can run a cell by either using the button \"play\" at the top bar or by using the \"shift + enter\" key combination\n",
    "    - Running a code cell executes it\n",
    "    - Running a text cell formats the text\n",
    "- Once you run a cell it stays in memory! So code will be run based on which order you execute cells, even if you execute a cell that is below another one before\n",
    "- General rule #1: try to arrange cell step-by-stop from top to bottom. If anything breaks, try to execute every cell from the top\n",
    "- General rule #2: if you are stuck or a cell is blocked during execution re-run the kernel from the top bar menu\n",
    " \n",
    "### Contents\n",
    "You can navigate through this exercise contents with the file explorer on the left.  \n",
    "The contents are \"extracted\" from the \n",
    "[TPCx-BB](https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) \n",
    "benchmark source folder. \n",
    "Please refer to the link if you want to have a broader overview and/or additional information TPCx-BB. \n",
    "Since this exercise differs from the actual benchmark, only a subset of its content are reported here:\n",
    "- `queries/` contains 30 SQL/Spark queries, some of which are to be ported to Spark in this exercise. Every query `qxx/` folder (`xx` = number) contains\n",
    "    - `engineLocalSettings.conf`: TPC related, disregard\n",
    "    - `engineLocalSettings.sql`: TPC related, disregard\n",
    "    - `explain_qxx.sql`: *query content* in \"explanatory\" format\n",
    "    - `qxx.sql`: *query content* in TPC exec format\n",
    "    - `run.sh`: TPC related, disregard\n",
    "    - `results/qxx-result`: contains the expect result in plain-text. You should compare this with your query output (example provided later)\n",
    "- `spark_table_schemas`: contains schema information for every table in the dataset. Not relevant for the implementation\n",
    "- `TPCx-BB-dataset`: contains all the tables in separate folder. Refer to it for table names\n",
    "\n",
    "**Do not modify** `spark_table_schemas` or `TPCx-BB-dataset` contents as it may compromise your solution.\n",
    "\n",
    "### Guidelines\n",
    "You must use the Spark SQL module to solve this exercise. Refer to the official documentation:\n",
    "> Spark SQL: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "We will work with *DataFrames*: a Spark data type used to represent collections of data, including database Tables. \n",
    "You are strongly recommended to refer to the DataFrame API reference within the Spark SQL module during the exercise implementation. \n",
    "There you will find methods, functions and further datatypes which are equivalent to SQL operations. \n",
    "> DataFrame Reference: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n",
    "The Spark DataFrame API resembles the one of Pandas library.\n",
    " \n",
    "Reference: \n",
    " [PySpark API documentation](https://spark.apache.org/docs/latest/api/python/reference/index.html)\n",
    "\n",
    "#### Reading TCPx-BB queries\n",
    "The SQL queries files (`explain_qxx.sql` and `qxx.sql`) are taken directly from the TPCx-BB benchmark suite \n",
    "and therefore might contain \"extra\" SQL statements and comments, \n",
    "which are functional to the TCPx-BB original benchmark (e.g. `hive` instructions, `EXPLAIN`, etc.). \n",
    "Your goal is to extract and translate the SQL query only, disregarding irrelevant statements/instructions for the purpose of this exercise.  \n",
    "Additionally, queries might contain *template* variables, in the form `${qxx_variable_name}`. \n",
    "You can find all relative templates in the `query/queryParameters.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Environment preparation\n",
    "**Make sure to read through and run the following code cells before starting the exercise** \n",
    "\n",
    "### Install PySpark\n",
    "\n",
    "Setup of python environment done through `pyproject.toml` specifications and \n",
    "[_poetry_](https://python-poetry.org/docs/)\n",
    "on project root directory.\n",
    "Follow `README.md` instructions to install the dependencies in a freshly created virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "\n",
    "Whenever working with Spark, you need to either start a Spark Session or join one.\n",
    "The Spark Session Builder will handle under-the-hood the architecture of the framework discussed in class \n",
    "and give us an entry point to programming with Spark.\n",
    "It will create an application UI panel at `localhost:4040` by default.\n",
    "Go check it to see info regarding driver, executors and jobs for the current configuration.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/28 22:22:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Homework 07</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1120dfa10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# when run locally, spark has one (master) node with its own jvm and no cluster manager is created\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Homework 07\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451c062-14f5-4c31-8d74-fa495f37e2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Spark API\n",
    "\n",
    "The building block of the Spark API is its Resilient Distributed Dataset (RDD) API.\n",
    "In the RDD API, there are two types of operations: transformations, which define a new dataset based on previous ones,\n",
    "and actions, which kick off a job to execute on a cluster.\n",
    "On top of Spark's RDD API, high level APIs are provided e.g. Dataframe API and Machine Learning API.\n",
    "We will focus on the former. \n",
    "\n",
    "\n",
    "# PySpark Dataframes\n",
    "\n",
    "Dataframes are a data structure for data manipulation.\n",
    "A [PySpark Dataframe](https://spark.apache.org/docs/latest/api/python/reference/index.html) \n",
    "is represented as a 2-dimensional labeled data structure with columns of potentially different types.\n",
    "Similar to what a spreadsheet or SQL table looks like.\n",
    "Most functionality and API of \n",
    "[Pandas](https://pandas.pydata.org/docs/)\n",
    "data analysis library is proposed in PySpark with support for distributed collections of data.\n",
    "\n",
    "To start, let's see how to create a dataframe, visualize the data in it and retrieve its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 33|\n",
      "|  2|    Bob| 45|\n",
      "|  3|Charlie| 50|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from scratch\n",
    "df = spark.createDataFrame(\n",
    "    data=[(1, \"Alice\", 33), (2, \"Bob\", 45), (3, \"Charlie\", 50)],\n",
    "    schema=[\"id\", \"name\", \"age\"],\n",
    ")\n",
    "\n",
    "# visualize dataframe representation\n",
    "df.show()  # by default shows first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pyspark.sql.classic.dataframe.DataFrame'>\n",
      "\n",
      "First top n=2 rows: [Row(id=1, name='Alice', age=33), Row(id=2, name='Bob', age=45)]\n",
      "\n",
      "Column title names: ['id', 'name', 'age']\n",
      "\n",
      "Column title names with its types: [('id', 'bigint'), ('name', 'string'), ('age', 'bigint')]\n",
      "\n",
      "Selecting a column: DataFrame[name: string]\n",
      "\n",
      "Avoid pandas notation (less features): Column<'name'>\n",
      "\n",
      "Selecting more than one columns: DataFrame[name: string, age: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Type: {type(df)}\",  # PySpark Dataframe is not equal to Pandas Dataframe !\n",
    "    f\"First top n=2 rows: {df.head(2)}\",  # rows returned in a list\n",
    "    f\"Column title names: {df.columns}\",\n",
    "    f\"Column title names with its types: {df.dtypes}\",\n",
    "    f'Selecting a column: {df.select(\"name\")}',  # it's still a DataFrame\n",
    "    f'Avoid pandas notation (less features): {df[\"name\"]}',  # Column object with lesser features than PySpark DataFrame\n",
    "    f'Selecting more than one columns: {df.select([\"name\", \"age\"])}',  # it's still a DataFrame\n",
    "    sep=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 33|\n",
      "|    Bob| 45|\n",
      "|Charlie| 50|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the values of the selected columns\n",
    "df.select([\"name\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+------------------+\n",
      "|summary| id|   name|               age|\n",
      "+-------+---+-------+------------------+\n",
      "|  count|  3|      3|                 3|\n",
      "|   mean|2.0|   NULL|42.666666666666664|\n",
      "| stddev|1.0|   NULL| 8.736894948054106|\n",
      "|    min|  1|  Alice|                33|\n",
      "|    max|  3|Charlie|                50|\n",
      "+-------+---+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# get statistics retrieved from a dataframe, computationally expensive, similar to Pandas\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Column operations on DataFrame\n",
    "\n",
    "Column operations generate a new DataFrame that need to be stored in a variable to be saved.\n",
    "It does not change the original DataFrame, but creates a new modified one from it.\n",
    "\n",
    "The following operations on column are presented:\n",
    "- creation\n",
    "- renaming\n",
    "- deletion\n",
    "\n",
    "The method `col('...')` is the one responsible \n",
    "to return a column given its name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------+\n",
      "| id|   name|age|age in 2 years|\n",
      "+---+-------+---+--------------+\n",
      "|  1|  Alice| 33|            35|\n",
      "|  2|    Bob| 45|            47|\n",
      "|  3|Charlie| 50|            52|\n",
      "+---+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a column where the 'age' is increase by +2\n",
    "df_col_added = df.withColumn(colName=\"age in 2 years\", col=col(\"age\") + 2)\n",
    "df_col_added.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n",
      "|   name|age|age in 2 years|\n",
      "+-------+---+--------------+\n",
      "|  Alice| 33|            35|\n",
      "|    Bob| 45|            47|\n",
      "|Charlie| 50|            52|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove the 'id' column\n",
    "df_col_removed = df_col_added.drop(\"id\")\n",
    "df_col_removed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------------+\n",
      "|name_column_renamed|age|age in 2 years|\n",
      "+-------------------+---+--------------+\n",
      "|              Alice| 33|            35|\n",
      "|                Bob| 45|            47|\n",
      "|            Charlie| 50|            52|\n",
      "+-------------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the 'name' column\n",
    "df_col_renamed = df_col_removed.withColumnRenamed(\n",
    "    existing=\"name\", new=\"name_column_renamed\"\n",
    ")\n",
    "\n",
    "df_col_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Row operations on DataFrame\n",
    "\n",
    "The result from this operations need to be saved in a new variable since it does not change the original dataframe. \n",
    "Same logic as column operations.\n",
    "\n",
    "The following operations on rows are presented:\n",
    "- creation\n",
    "- update\n",
    "- deletion\n",
    "\n",
    "The examples provided are scoped within the context of handling null missing values in dataframe records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "|  id|   name| age|\n",
      "+----+-------+----+\n",
      "|   1|  Alice|  33|\n",
      "|NULL|    Bob|  45|\n",
      "|   3|Charlie|NULL|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update a row setting a null value\n",
    "df_null = df.withColumn(\n",
    "    colName=\"age\", col=when(col(\"age\") >= 50, None).otherwise(col(\"age\"))\n",
    ")\n",
    "\n",
    "df_null = df_null.withColumn(\n",
    "    colName=\"id\", col=when(col(\"Name\") == \"Bob\", None).otherwise(col(\"id\"))\n",
    ")\n",
    "\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Missing values are generally referred as `NA`, which is a sentinel value.\n",
    "Further explanation in \n",
    "[pandas](https://pandas.pydata.org/docs/user_guide/missing_data.html#missing-data),\n",
    "[pyspark](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.na.html?highlight=na#pyspark.sql.DataFrame.na)\n",
    "specific docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 33|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove rows if containing any null value in field attributes\n",
    "df_without_null_rows = df_null.dropna()\n",
    "# same as df_null.na.drop(), df_null.dropna(how='any'), df_null.na.drop(how='any')\n",
    "df_without_null_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1|  Alice|  33|\n",
      "|  3|Charlie|NULL|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove rows only if null missing values is present in the specified column\n",
    "df_without_id_null = df_null.dropna(subset=[\"id\"])\n",
    "df_without_id_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 33|\n",
      "|  3|Charlie| -1|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace null missing values with the one provided, must correspond to column type\n",
    "df_filled_nulls = df_without_id_null.fillna(-1)\n",
    "df_filled_nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Filter operations\n",
    "\n",
    "Fundamental to implement conditions on data.\n",
    "We pick again our old non-modified dataframe.\n",
    "The 3 fundamental boolean operators for filters are:\n",
    "- & and\n",
    "- | or\n",
    "- ~ not\n",
    "\n",
    "Remember to put brackets for more than one condition inside filter\n",
    "e.g. `.filter((...) & (...) | (...))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  2| Bob| 45|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_people_forty = df.filter(condition=(40 <= df.age) & (df.age < 50))\n",
    "df_people_forty.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  2| 45|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remember the select operation presented at the start to chose only relevant columns\n",
    "df_people_forty_anonymized = df_people_forty.select([\"id\", \"age\"])\n",
    "df_people_forty_anonymized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "Join columns of another Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|faculty|\n",
      "+---+-------+\n",
      "|  1|    INF|\n",
      "|  2|    ECO|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new faculties dataframe\n",
    "df_faculties = spark.createDataFrame(\n",
    "    data=[(1, \"INF\"), (2, \"ECO\")],\n",
    "    schema=[\"id\", \"faculty\"],\n",
    ")\n",
    "\n",
    "df_faculties.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+\n",
      "| id|   name|age|faculty|\n",
      "+---+-------+---+-------+\n",
      "|  1|  Alice| 33|      1|\n",
      "|  2|    Bob| 45|      2|\n",
      "|  3|Charlie| 50|      1|\n",
      "+---+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a column to the original df to perform join\n",
    "df_faculty_assigned = df.withColumn(\n",
    "    colName=\"faculty\", col=when(df.name == \"Bob\", 2).otherwise(\"1\")\n",
    ")\n",
    "\n",
    "df_faculty_assigned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------+---+-------+\n",
      "| id|   name|age|faculty| id|faculty|\n",
      "+---+-------+---+-------+---+-------+\n",
      "|  1|  Alice| 33|      1|  1|    INF|\n",
      "|  3|Charlie| 50|      1|  1|    INF|\n",
      "|  2|    Bob| 45|      2|  2|    ECO|\n",
      "+---+-------+---+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOIN operation\n",
    "df_joined = df_faculty_assigned.join(\n",
    "    other=df_faculties, on=df_faculty_assigned.faculty == df_faculties.id\n",
    ")\n",
    "\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Aggregates\n",
    "\n",
    "GroupBy functionality is implemented as the method `.groupBy()` in spark dataframe API.\n",
    "It then allows you to use aggregate functionality `.agg()` on the resulting object \n",
    "that can perform any aggregate operation you have already seen in SQL.\n",
    "It involves a combination of splitting the object, applying a function on the data \n",
    "and recombine the result.\n",
    "Example of aggregate operations available: `.count()`, `.sum()`, `.mean()`, `.max()`, `.min()`, ...\n",
    "Do not use the alternative with lowercase letters `.groupby()` as it is for pandas compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|faculty|avg(age)|\n",
      "+-------+--------+\n",
      "|      1|    41.5|\n",
      "|      2|    45.0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the average age of the faculty\n",
    "df_faculty_avg_age = df_faculty_assigned.groupBy(df_faculty_assigned.faculty).agg(\n",
    "    avg(df_faculty_assigned.age)\n",
    ")\n",
    "\n",
    "df_faculty_avg_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Alias\n",
    "\n",
    "Correspondent of 'AS' sql keyword. \n",
    "Allow data structure to be referenced using an alternative name.\n",
    "Can be applied to dataframes, columns.\n",
    "If in the same expression you are renaming a column \n",
    "and want to use it in another function, \n",
    "reference to such column with `col('aliasedName')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 33|\n",
      "|  2|    Bob| 45|\n",
      "|  3|Charlie| 50|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_alias = df.alias(\"df_alias\")\n",
    "df_alias.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|faculty|Average Age|\n",
      "+-------+-----------+\n",
      "|      1|       41.5|\n",
      "|      2|       45.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from previous example of aggregates\n",
    "df_faculty_avg_age = df_faculty_assigned.groupBy(df_faculty_assigned.faculty).agg(\n",
    "    avg(df_faculty_assigned.age).alias(\"Average Age\")\n",
    ")\n",
    "\n",
    "df_faculty_avg_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## OrderBy\n",
    "\n",
    "Returns a new sorted dataframe by the specified column(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|faculty|Average Age|\n",
      "+-------+-----------+\n",
      "|      2|       45.0|\n",
      "|      1|       41.5|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_faculty_avg_age.orderBy(desc(col=\"Average Age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Others\n",
    "\n",
    "Important functions from SQL have their own correspondent, \n",
    "you should be able to complete the assignment with the ones listed.\n",
    "Since there are many solutions to reach the same goal \n",
    "for the query translation exercise, \n",
    "just check out the documentation for further references:\n",
    "[PySpark Dataframe API](https://spark.apache.org/docs/latest/api/python/reference/index.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Define Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table from TPCxx-BB dataset. Returning a dataframe read from parquet format\n",
    "get_table = lambda table: spark.read.option(\"header\", \"true\").parquet(\n",
    "    f\"TPCx-BB-dataset/{table}.ptxt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "\n",
    "You can use `get_table` to load current dataset tables. A table in Spark is stored as a *DataFrame* - see reference in the exercise intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "|c_customer_sk|   c_customer_id|c_current_cdemo_sk|c_current_hdemo_sk|c_current_addr_sk|c_first_shipto_date_sk|c_first_sales_date_sk|c_salutation|c_first_name|c_last_name|c_preferred_cust_flag|c_birth_day|c_birth_month|c_birth_year|     c_birth_country|      c_login|     c_email_address|c_last_review_date|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "|            0|AAAAAAAAAAAAAAAA|           1824793|              3203|             2555|                 28776|                14690|         Ms.|      Marisa| Harrington|                    N|         17|            4|        1988|UNITED ARAB EMIRATES| RRCyuY3XfE3a|Marisa.Harrington...|          gdMmGdU9|\n",
      "|            1|AAAAAAAAAAAAAAAB|            830976|              2600|             9191|                 94658|                19931|        Miss|      Bessie|   Calderon|                    N|         20|           10|        1945|              MONACO|      aCJb9cc|Bessie.Calderon@s...|         hLQn0LtVa|\n",
      "|            2|AAAAAAAAAAAAAAAC|            335540|              5527|            14091|                 31783|               106195|        Miss|     Barbara|     Hoover|                    Y|          3|            4|        1952|           GREENLAND|   59cfL9Fbpv|Barbara.Hoover@la...|          vDVbSOCI|\n",
      "|            3|AAAAAAAAAAAAAAAD|           1056662|              1978|            19122|                  8741|                87583|         Mr.|       Lanny|       Berg|                    N|          2|            1|        1932|              CANADA|    TvsKkZgPt|   Lanny.Berg@gmx.fr|          ZxCh4LKH|\n",
      "|            4|AAAAAAAAAAAAAAAE|            596707|              3971|            21996|                 14688|                75952|        Mrs.|        Gina|      Felix|                    Y|         18|            4|        1959|               NAURU|     t3r2t4rC|Gina.Felix@amplim...|            WpJjJ1|\n",
      "|            5|AAAAAAAAAAAAAAAF|            363428|              4888|            21272|                 38182|                65826|         Dr.|      Judith|     Mccain|                    N|         11|           10|        1936|             ICELAND|          WfT|Judith.Mccain@pri...|          jXmMzRt8|\n",
      "|            6|AAAAAAAAAAAAAAAG|           1601629|              6779|            18776|                 48020|                75950|         Sir|      Lavern|     Guzman|                    N|          2|            7|        1952|                GUAM|     SD7IbZaQ|Lavern.Guzman@pea...|            xKpEnQ|\n",
      "|            7|AAAAAAAAAAAAAAAH|            260357|              2400|            30589|                 20361|                42197|         Dr.|      Janice|     Murray|                    Y|         23|            8|        1976|           LITHUANIA|   p1FXddH8rW|Janice.Murray@mac...|         xhjGNk98a|\n",
      "|            8|AAAAAAAAAAAAAAAI|           1519111|              1818|            23037|                 39194|                62551|         Mr.|         Lee|    Jenkins|                    N|         15|           12|        1972|             TUNISIA|         s2Ef|Lee.Jenkins@hushm...|                 j|\n",
      "|            9|AAAAAAAAAAAAAAAJ|            555297|              5958|            30815|                 43531|                50147|         Mr.|    Jonathan|     Parker|                    Y|         10|            2|        1976|             GEORGIA|       N2Qq55|Jonathan.Parker@m...|            vgudeV|\n",
      "|           10|AAAAAAAAAAAAAAAK|            890454|              5535|            34947|                 78774|               102861|         Sir|      George|   Anderson|                    N|         14|            3|        1968|             TUNISIA|           Sa|George.Anderson@s...|                46|\n",
      "|           11|AAAAAAAAAAAAAAAL|           1438986|              1495|              761|                 91923|                70592|         Sir|       Jorge|     Taylor|                    N|         22|            2|        1963|               NIGER|zIhZa6uiGHzb6| Jorge.Taylor@ip6.li|        bmZgmTmXHY|\n",
      "|           12|AAAAAAAAAAAAAAAM|           1425262|              1525|            44513|                 10286|               103343|         Dr.|      Cheryl|        Lee|                    Y|         12|           11|        1972|             JAMAICA|     tBf8lG0t|Cheryl.Lee@lavabi...|             h3tgo|\n",
      "|           13|AAAAAAAAAAAAAAAN|           1331330|              2300|            13401|                 51231|               102876|         Mr.|         Don|   Williams|                    Y|         15|            7|        1952|              BRAZIL|     cb00ffNo|Don.Williams@gmx....|        aEV7ed7nMQ|\n",
      "|           14|AAAAAAAAAAAAAAAO|           1633793|              5097|            21014|                 71728|                 1803|         Ms.|      Alicia|     Flores|                    Y|         15|           10|        1983|         AFGHANISTAN|          Y3e|Alicia.Flores@gmx.hk|         rsOpe2e5s|\n",
      "|           15|AAAAAAAAAAAAAAAP|            686483|              2725|            25060|                 44067|                61458|         Sir|        Paul|    Becerra|                    N|         13|            2|        1955|        COOK ISLANDS|  eILOny1Kpzq|Paul.Becerra@gmai...|          EGo3wwNs|\n",
      "|           16|AAAAAAAAAAAAAAAQ|            885432|               808|            42429|                 58839|                53580|         Sir|       Ellis| Cunningham|                    Y|         19|            6|        1981|                CUBA|            L|Ellis.Cunningham@...|                hw|\n",
      "|           17|AAAAAAAAAAAAAAAR|           1536897|              1789|            10433|                 34329|                88531|         Mr.|       Lance|     Hardee|                    Y|          2|            2|        1982|SYRIAN ARAB REPUBLIC|          1iF| Lance.Hardee@ip6.li|            XPcaEU|\n",
      "|           18|AAAAAAAAAAAAAAAS|           1025955|              7158|            47874|                 93009|                88336|        Miss|      Alisha|    Jenkins|                    N|          4|           11|        1927|         ISLE OF MAN|  VTZFhF0rDxe|Alisha.Jenkins@we...|          czjma9gS|\n",
      "|           19|AAAAAAAAAAAAAAAT|           1384075|              3101|            32881|                 15482|                52298|        Mrs.|        Adam|    Quigley|                    N|         14|            7|        1966|               CHINA|  LXkGj0iRk5X|Adam.Quigley@vfem...|         IwvFu8IYR|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# load the current table\n",
    "customer = get_table(\"customer\")\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "|c_customer_sk|   c_customer_id|c_current_cdemo_sk|c_current_hdemo_sk|c_current_addr_sk|c_first_shipto_date_sk|c_first_sales_date_sk|c_salutation|c_first_name|c_last_name|c_preferred_cust_flag|c_birth_day|c_birth_month|c_birth_year|     c_birth_country|     c_login|     c_email_address|c_last_review_date|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "|            0|AAAAAAAAAAAAAAAA|           1824793|              3203|             2555|                 28776|                14690|         Ms.|      Marisa| Harrington|                    N|         17|            4|        1988|UNITED ARAB EMIRATES|RRCyuY3XfE3a|Marisa.Harrington...|          gdMmGdU9|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# show the 1st row of the customer table\n",
    "customer.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('c_customer_sk', LongType(), True), StructField('c_customer_id', StringType(), True), StructField('c_current_cdemo_sk', LongType(), True), StructField('c_current_hdemo_sk', LongType(), True), StructField('c_current_addr_sk', LongType(), True), StructField('c_first_shipto_date_sk', LongType(), True), StructField('c_first_sales_date_sk', LongType(), True), StructField('c_salutation', StringType(), True), StructField('c_first_name', StringType(), True), StructField('c_last_name', StringType(), True), StructField('c_preferred_cust_flag', StringType(), True), StructField('c_birth_day', LongType(), True), StructField('c_birth_month', LongType(), True), StructField('c_birth_year', LongType(), True), StructField('c_birth_country', StringType(), True), StructField('c_login', StringType(), True), StructField('c_email_address', StringType(), True), StructField('c_last_review_date', StringType(), True)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the table schema, which in Spark is a set of [column, type, nullable]\n",
    "customer.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c_customer_sk: long (nullable = true)\n",
      " |-- c_customer_id: string (nullable = true)\n",
      " |-- c_current_cdemo_sk: long (nullable = true)\n",
      " |-- c_current_hdemo_sk: long (nullable = true)\n",
      " |-- c_current_addr_sk: long (nullable = true)\n",
      " |-- c_first_shipto_date_sk: long (nullable = true)\n",
      " |-- c_first_sales_date_sk: long (nullable = true)\n",
      " |-- c_salutation: string (nullable = true)\n",
      " |-- c_first_name: string (nullable = true)\n",
      " |-- c_last_name: string (nullable = true)\n",
      " |-- c_preferred_cust_flag: string (nullable = true)\n",
      " |-- c_birth_day: long (nullable = true)\n",
      " |-- c_birth_month: long (nullable = true)\n",
      " |-- c_birth_year: long (nullable = true)\n",
      " |-- c_birth_country: string (nullable = true)\n",
      " |-- c_login: string (nullable = true)\n",
      " |-- c_email_address: string (nullable = true)\n",
      " |-- c_last_review_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or for a nice pretty print tree view of it\n",
    "customer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Sample query translation\n",
    "Refer to `queries/q00/explain_q00.sql`. The code below is a valid translation of that query using SparkSQL. You can use any methods in the Spark SQL DataFrame class to implement your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Query 0\n",
    "Find the amount of items sold by their category.  \n",
    "Only in certain categories sold in specific stores are considered,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    i_category|count|\n",
      "+--------------+-----+\n",
      "|Home & Kitchen| 1975|\n",
      "|         Books|14455|\n",
      "|         Music|25060|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look into TCPx-BB-dataset/ directory to check all the available tables.\n",
    "# gather tables needed\n",
    "\n",
    "s = get_table(\"store_sales\")\n",
    "i = get_table(\"item\")\n",
    "\n",
    "q01_i_category_id_IN = [1, 2, 3]\n",
    "q01_ss_store_sk_IN = [10, 20, 33, 40, 50]\n",
    "\n",
    "query0_solution = (\n",
    "    s.join(other=i, on=s.ss_item_sk == i.i_item_sk)\n",
    "    .filter(condition=i.i_category_id.isin(q01_i_category_id_IN))\n",
    "    .filter(condition=s.ss_store_sk.isin(q01_ss_store_sk_IN))\n",
    "    .groupBy(i.i_category)\n",
    "    .count()\n",
    "    .select(\"i_category\", \"count\")\n",
    ")\n",
    "\n",
    "query0_solution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "The cell below is a shortcut to display the results file of q00 without navigating to the file.  \n",
    "The `!` symbol followed by a bash command (`cat` in this case) can be used as in-cell access to the terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home & Kitchen, 1975\n",
      "Books, 14455\n",
      "Music, 25060"
     ]
    }
   ],
   "source": [
    "## check the result\n",
    "!cat queries/q00/results/q00-result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# [YOUR SOLUTION BELOW]\n",
    "Write the query description in a Markdown cell, followed by a code cell with the query implementation.  \n",
    "Query descriptions can be found in the TCPx-BB specification, page 93: https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp\n",
    "and in the _/queries_ folder of this exercise.\n",
    "\n",
    "You should implement all the queries assigned in the homework sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## 1) Query 07\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dateRange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m c_a_s=c_a.join(other = s, on = c.c_customer_sk == s.ss_customer_sk) \u001b[38;5;66;03m#c_a+store_sales\u001b[39;00m\n\u001b[32m     23\u001b[39m c_a_s_o=c_a_s.join(other = overpriced, on=s.ss_item_sk==overpriced.i_item_sk)\u001b[38;5;66;03m#c_a_s+overpriced\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m casod=c_a_s_o.join(other = \u001b[43mdateRange\u001b[49m, on=s.ss_sold_date_sk==dateRange.d_date_sk)\n\u001b[32m     26\u001b[39m query7_solution= casod.groupBy(\u001b[33m\"\u001b[39m\u001b[33mca_state\u001b[39m\u001b[33m\"\u001b[39m).count().alias(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m).filter(\n\u001b[32m     27\u001b[39m     condition=col(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m) >= \u001b[38;5;28mmax\u001b[39m).orderBy(desc(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mca_state\u001b[39m\u001b[33m\"\u001b[39m).limit(\u001b[38;5;28mmax\u001b[39m).select(\u001b[33m\"\u001b[39m\u001b[33mca_state\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m query7_solution.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'dateRange' is not defined"
     ]
    }
   ],
   "source": [
    "year=2004\n",
    "month=7\n",
    "max=10\n",
    "\n",
    "j=get_table(\"item\").withColumn(colName=\"more\", col=col(\"i_current_price\")*1.2 )\n",
    "\n",
    "avg_Cat_Prices = j.groupBy(j.i_category).agg(avg(j.more).alias(\"Average+20%\"))\n",
    "\n",
    "k=get_table(\"item\")\n",
    "overpriced =k.join(other=avg_Cat_Prices, on=(k[\"i_category\"] == avg_Cat_Prices[\"i_category\"]) &(k[\"i_current_price\"] > avg_Cat_Prices[\"Average+20%\"]))\n",
    "    \n",
    "\n",
    "d=get_table(\"date_dim\").select(\"d_date_sk\", \"d_year\", \"d_moy\")\n",
    "date_range= d.filter(condition = (col(\"d_year\") == year) & (col(\"d_moy\") == month)) .select(\"d_date_sk\")\n",
    "\n",
    "a= get_table(\"customer_address\")\n",
    "c= get_table(\"customer\")\n",
    "c_a = a.join(other = c, on = a.ca_address_sk == c.c_current_addr_sk)#customer +address\n",
    "\n",
    "s= get_table(\"store_sales\")  \n",
    "c_a_s=c_a.join(other = s, on = c.c_customer_sk == s.ss_customer_sk) #c_a+store_sales\n",
    "\n",
    "c_a_s_o=c_a_s.join(other = overpriced, on=s.ss_item_sk==overpriced.i_item_sk)#c_a_s+overpriced\n",
    "casod=c_a_s_o.join(other = dateRange, on=s.ss_sold_date_sk==dateRange.d_date_sk)\n",
    "\n",
    "query7_solution= casod.groupBy(\"ca_state\").count().alias(\"count\").filter(\n",
    "    condition=col(\"count\") >= max).orderBy(desc(\"count\"), \"ca_state\").limit(max).select(\"ca_state\", \"count\")\n",
    "\n",
    "query7_solution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "!cat queries/q07/results/q07-result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## 2.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## 2.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## 2.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## 2.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## 3) Query 09\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "!cat queries/q09/results/q09-result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## 4) Query 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `condition` should be a Column or str, got bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m q09_part3_sales_price_min=\u001b[32m150\u001b[39m\n\u001b[32m     35\u001b[39m q09_part3_sales_price_max=\u001b[32m200\u001b[39m\n\u001b[32m     37\u001b[39m solution = (\n\u001b[32m     38\u001b[39m     ss1.join(\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         other = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md_year\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mq09_year\u001b[49m\u001b[43m)\u001b[49m, on = (ss1.ss_sold_date_sk == dd.d_date_sk))\n\u001b[32m     40\u001b[39m         .join(\n\u001b[32m     41\u001b[39m             other = ca1, on = (ss1.ss_addr_sk == ca1.ca_address_sk))\n\u001b[32m     42\u001b[39m         .join(\n\u001b[32m     43\u001b[39m             other = s, on = (s.s_store_sk == ss1.ss_store_sk))\n\u001b[32m     44\u001b[39m         .join(other = cd, on = (cd.cd_demo_sk == ss1.ss_cdemo_sk))\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m         .filter(\n\u001b[32m     47\u001b[39m             (\n\u001b[32m     48\u001b[39m                 (\n\u001b[32m     49\u001b[39m                 (cd.cd_marital_status == q09_part1_marital_status)\n\u001b[32m     50\u001b[39m                 & (cd.cd_education_status == q09_part1_education_status)\n\u001b[32m     51\u001b[39m                 & (q09_part1_sales_price_min <= ss1.ss_sales_price)\n\u001b[32m     52\u001b[39m                 & (ss1.ss_sales_price <= q09_part1_sales_price_max)\n\u001b[32m     53\u001b[39m                 )\n\u001b[32m     54\u001b[39m                 |\n\u001b[32m     55\u001b[39m                 (\n\u001b[32m     56\u001b[39m                 cd.cd_marital_status == q09_part2_marital_status\n\u001b[32m     57\u001b[39m                 & cd.cd_education_status == q09_part2_education_status\n\u001b[32m     58\u001b[39m                 & q09_part2_sales_price_min <= ss1.ss_sales_price\n\u001b[32m     59\u001b[39m                 & ss1.ss_sales_price <= q09_part2_sales_price_max\n\u001b[32m     60\u001b[39m                 )\n\u001b[32m     61\u001b[39m                 |\n\u001b[32m     62\u001b[39m                 (\n\u001b[32m     63\u001b[39m                 cd.cd_marital_status == q09_part3_marital_status\n\u001b[32m     64\u001b[39m                 & cd.cd_education_status == q09_part3_education_status\n\u001b[32m     65\u001b[39m                 & q09_part3_sales_price_min <= ss1.ss_sales_price\n\u001b[32m     66\u001b[39m                 & ss1.ss_sales_price <= q09_part3_sales_price_max\n\u001b[32m     67\u001b[39m                 )\n\u001b[32m     68\u001b[39m             )\n\u001b[32m     69\u001b[39m             &\n\u001b[32m     70\u001b[39m             (\n\u001b[32m     71\u001b[39m                (\n\u001b[32m     72\u001b[39m                 ca1.ca_country == q09_part1_ca_country\n\u001b[32m     73\u001b[39m                 & ca1.ca_state.isin (q09_part1_ca_state_IN)\n\u001b[32m     74\u001b[39m                 & q09_part1_net_profit_min <= ss1.ss_net_profit\n\u001b[32m     75\u001b[39m                 & ss1.ss_net_profit <= q09_part1_net_profit_max\n\u001b[32m     76\u001b[39m                 )\n\u001b[32m     77\u001b[39m                 |\n\u001b[32m     78\u001b[39m                 (\n\u001b[32m     79\u001b[39m                 ca1.ca_country == q09_part2_ca_country\n\u001b[32m     80\u001b[39m                 & ca1.ca_state.isin (q09_part2_ca_state_IN)\n\u001b[32m     81\u001b[39m                 & q09_part2_net_profit_min <= ss1.ss_net_profit\n\u001b[32m     82\u001b[39m                 & ss1.ss_net_profit <= q09_part2_net_profit_max\n\u001b[32m     83\u001b[39m                 )\n\u001b[32m     84\u001b[39m                 |\n\u001b[32m     85\u001b[39m                 (\n\u001b[32m     86\u001b[39m                 ca1.ca_country == q09_part3_ca_country\n\u001b[32m     87\u001b[39m                 & ca1.ca_state.isin (q09_part3_ca_state_IN)\n\u001b[32m     88\u001b[39m                 & q09_part3_net_profit_min <= ss1.ss_net_profit\n\u001b[32m     89\u001b[39m                 & ss1.ss_net_profit <= q09_part3_net_profit_max\n\u001b[32m     90\u001b[39m                 ) \n\u001b[32m     91\u001b[39m             )\n\u001b[32m     92\u001b[39m             )\n\u001b[32m     93\u001b[39m             .select(\u001b[38;5;28msum\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mss_quantity\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     94\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:1014\u001b[39m, in \u001b[36mDataFrame.filter\u001b[39m\u001b[34m(self, condition)\u001b[39m\n\u001b[32m   1012\u001b[39m     jdf = \u001b[38;5;28mself\u001b[39m._jdf.filter(condition._jc)\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1015\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1016\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1017\u001b[39m     )\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN_OR_STR] Argument `condition` should be a Column or str, got bool."
     ]
    }
   ],
   "source": [
    "# implementation\n",
    "ss1 = get_table(\"store_sales\")\n",
    "dd = get_table(\"date_dim\")\n",
    "ca1 = get_table(\"customer_address\")\n",
    "s = get_table(\"store\")\n",
    "cd = get_table(\"customer_demographics\")\n",
    "\n",
    "q09_year=2001 \n",
    "\n",
    "q09_part1_ca_country=\"United States\"\n",
    "q09_part1_ca_state_IN=['KY', 'GA', 'NM']\n",
    "q09_part1_net_profit_min=0\n",
    "q09_part1_net_profit_max=2000\n",
    "q09_part1_education_status=\"4 yr Degree\"\n",
    "q09_part1_marital_status=\"M\"\n",
    "q09_part1_sales_price_min=100\n",
    "q09_part1_sales_price_max=150\n",
    "\n",
    "q09_part2_ca_country=\"United States\"\n",
    "q09_part2_ca_state_IN=['MT', 'OR', 'IN']\n",
    "q09_part2_net_profit_min=150\n",
    "q09_part2_net_profit_max=3000\n",
    "q09_part2_education_status=\"4 yr Degree\"\n",
    "q09_part2_marital_status=\"M\"\n",
    "q09_part2_sales_price_min=50\n",
    "q09_part2_sales_price_max=200\n",
    "\n",
    "q09_part3_ca_country=\"United States\"\n",
    "q09_part3_ca_state_IN=['WI', 'MO', 'WV']\n",
    "q09_part3_net_profit_min=50\n",
    "q09_part3_net_profit_max=25000\n",
    "q09_part3_education_status=\"4 yr Degree\"\n",
    "q09_part3_marital_status=\"M\"\n",
    "q09_part3_sales_price_min=150\n",
    "q09_part3_sales_price_max=200\n",
    "\n",
    "solution = (\n",
    "    ss1.join(\n",
    "        other = dd.filter('d_year' == q09_year), on = (ss1.ss_sold_date_sk == dd.d_date_sk))\n",
    "        .join(\n",
    "            other = ca1, on = (ss1.ss_addr_sk == ca1.ca_address_sk))\n",
    "        .join(\n",
    "            other = s, on = (s.s_store_sk == ss1.ss_store_sk))\n",
    "        .join(other = cd, on = (cd.cd_demo_sk == ss1.ss_cdemo_sk))\n",
    "\n",
    "        .filter(\n",
    "            (\n",
    "                (\n",
    "                (cd.cd_marital_status == q09_part1_marital_status)\n",
    "                & (cd.cd_education_status == q09_part1_education_status)\n",
    "                & (q09_part1_sales_price_min <= ss1.ss_sales_price)\n",
    "                & (ss1.ss_sales_price <= q09_part1_sales_price_max)\n",
    "                )\n",
    "                |\n",
    "                (\n",
    "                cd.cd_marital_status == q09_part2_marital_status\n",
    "                & cd.cd_education_status == q09_part2_education_status\n",
    "                & q09_part2_sales_price_min <= ss1.ss_sales_price\n",
    "                & ss1.ss_sales_price <= q09_part2_sales_price_max\n",
    "                )\n",
    "                |\n",
    "                (\n",
    "                cd.cd_marital_status == q09_part3_marital_status\n",
    "                & cd.cd_education_status == q09_part3_education_status\n",
    "                & q09_part3_sales_price_min <= ss1.ss_sales_price\n",
    "                & ss1.ss_sales_price <= q09_part3_sales_price_max\n",
    "                )\n",
    "            )\n",
    "            &\n",
    "            (\n",
    "               (\n",
    "                ca1.ca_country == q09_part1_ca_country\n",
    "                & ca1.ca_state.isin (q09_part1_ca_state_IN)\n",
    "                & q09_part1_net_profit_min <= ss1.ss_net_profit\n",
    "                & ss1.ss_net_profit <= q09_part1_net_profit_max\n",
    "                )\n",
    "                |\n",
    "                (\n",
    "                ca1.ca_country == q09_part2_ca_country\n",
    "                & ca1.ca_state.isin (q09_part2_ca_state_IN)\n",
    "                & q09_part2_net_profit_min <= ss1.ss_net_profit\n",
    "                & ss1.ss_net_profit <= q09_part2_net_profit_max\n",
    "                )\n",
    "                |\n",
    "                (\n",
    "                ca1.ca_country == q09_part3_ca_country\n",
    "                & ca1.ca_state.isin (q09_part3_ca_state_IN)\n",
    "                & q09_part3_net_profit_min <= ss1.ss_net_profit\n",
    "                & ss1.ss_net_profit <= q09_part3_net_profit_max\n",
    "                ) \n",
    "            )\n",
    "            )\n",
    "            .select(sum(\"ss_quantity\"))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "!cat queries/q20/results/q20-result-queryonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource saver: gracefully stop the spark session :)\n",
    "# spark.stop()\n",
    "\n",
    "# hope to see you using clusters with Spark in Databricks platform\n",
    "# on the cloud as AWS, Azure, GCP\n",
    "# for your Data Engineering projects :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72b3c9-33d1-4172-b3dc-1b1e9f19f17a",
   "metadata": {},
   "source": [
    "## 5) Query 26??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f99e1-e6cf-4f97-856d-dad4b5aeec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "category=\"Books\"\n",
    "count=5\n",
    "ss=get_table(\"store_sales\")\n",
    "i = get_table(\"item\")\n",
    "\n",
    "join1= ss.join(other=i, on= ss.ss_item_sk == i.i_item_sk).filter(condition= i.i_category==category).dropna(subset=[\"ss_customer_sk\"])\n",
    "join2=join1.groupBy(ss.ss_customer_sk).count().alias(\"count\").filter(condition=col(\"count\") >= count)\n",
    "# join2.orderBy(asc(col=\"ss_customer_sk\")).select(\"ss_customer_sk\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9ec90-076c-461a-9ab8-3b3f3a3b6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "# !cat queries/q26/results/q26-result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a207e8-45b4-48fc-9197-3ad4d86c4545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e1bf84d-9c55-4f29-b8cb-e00e4059a1bb",
   "metadata": {},
   "source": [
    "## 6) Query11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45e4737f-fcb4-4737-8791-0dd3f667741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+-----+\n",
      "|pr_item_sk|        avg_rating|pr_item_sk|count|\n",
      "+----------+------------------+----------+-----+\n",
      "|     16896|               4.0|     16896|    3|\n",
      "|     15371|               5.0|     15371|    2|\n",
      "|     11945|               5.0|     11945|    2|\n",
      "|     10422|               4.2|     10422|    5|\n",
      "|      1806| 4.333333333333333|      1806|    6|\n",
      "|     14719|               4.5|     14719|    2|\n",
      "|     16597|               5.0|     16597|    1|\n",
      "|      9458|               4.0|      9458|    3|\n",
      "|     17499|               4.6|     17499|    5|\n",
      "|       474|               5.0|       474|    4|\n",
      "|     14846| 4.166666666666667|     14846|    6|\n",
      "|      3506|               4.0|      3506|    3|\n",
      "|     15437|              3.25|     15437|    4|\n",
      "|       964|3.6666666666666665|       964|    3|\n",
      "|     14117|               5.0|     14117|    2|\n",
      "|     13518|               4.5|     13518|    6|\n",
      "|     13098|               4.5|     13098|    2|\n",
      "|      2250|               4.0|      2250|    5|\n",
      "|     11567|               4.6|     11567|    5|\n",
      "|      8075|               4.0|      8075|    2|\n",
      "+----------+------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- wr_returned_date_sk: long (nullable = true)\n",
      " |-- wr_returned_time_sk: long (nullable = true)\n",
      " |-- wr_item_sk: long (nullable = true)\n",
      " |-- wr_refunded_customer_sk: long (nullable = true)\n",
      " |-- wr_refunded_cdemo_sk: long (nullable = true)\n",
      " |-- wr_refunded_hdemo_sk: long (nullable = true)\n",
      " |-- wr_refunded_addr_sk: long (nullable = true)\n",
      " |-- wr_returning_customer_sk: long (nullable = true)\n",
      " |-- wr_returning_cdemo_sk: long (nullable = true)\n",
      " |-- wr_returning_hdemo_sk: long (nullable = true)\n",
      " |-- wr_returning_addr_sk: long (nullable = true)\n",
      " |-- wr_webpage_sk: long (nullable = true)\n",
      " |-- wr_reason_sk: long (nullable = true)\n",
      " |-- wr_order_number: long (nullable = true)\n",
      " |-- wr_return_quantity: long (nullable = true)\n",
      " |-- wr_return_amt: double (nullable = true)\n",
      " |-- wr_return_tax: double (nullable = true)\n",
      " |-- wr_return_amt_inc_tax: double (nullable = true)\n",
      " |-- wr_fee: double (nullable = true)\n",
      " |-- wr_return_ship_cost: double (nullable = true)\n",
      " |-- wr_refunded_cost: double (nullable = true)\n",
      " |-- wr_reversed_charge: double (nullable = true)\n",
      " |-- wr_account_credit: double (nullable = true)\n",
      " |-- wr_net_loss: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr_item= get_table(\"product_reviews\").dropna(subset=\"pr_item_sk\")\n",
    "# pr_item_sk.show()\n",
    "r_c= pr_item.groupBy(\"pr_item_sk\").count().alias(\"count\").select(\"pr_item_sk\", \"count\")\n",
    "p=pr_item.groupBy(\"pr_item_sk\").agg(avg(pr_item.pr_review_rating).alias(\"avg_rating\")).join(other=r_c, on=pr_item.pr_item_sk==r_c.pr_item_sk) \n",
    "p.show()\n",
    "\n",
    "startDate='2003-01-02'\n",
    "endDate='2003-02-02'\n",
    "\n",
    "ws= get_table(\"web_sales\")\n",
    "\n",
    "# ws_sum= ws.groupBy(\"ws_item_sk\").agg(sum(ws.ws_net_paid))\n",
    "\n",
    "d= get_table(\"date_dim\")\n",
    "get_table(\"web_returns\").printSchema()\n",
    "dd=d.filter(condition= (d.d_date >= startDate)&(d.d_date <= endDate))\n",
    "# sales_in_range=ws.join(other=date, on= ( ws.ws_sold_date_sk == dd.d_date_sk ))\n",
    "\n",
    "# .dropna(subset=\"ws_item_sk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1d418-13aa-4991-b117-623be470cd27",
   "metadata": {},
   "source": [
    "SELECT corr(reviews_count,avg_rating)\n",
    "FROM (\n",
    "  SELECT\n",
    "    p.pr_item_sk AS pid,\n",
    "    p.r_count    AS reviews_count,\n",
    "    p.avg_rating AS avg_rating,\n",
    "    s.revenue    AS m_revenue\n",
    "  FROM (\n",
    "    SELECT\n",
    "      pr_item_sk,\n",
    "      count(*) AS r_count,\n",
    "      avg(pr_review_rating) AS avg_rating\n",
    "    FROM product_reviews\n",
    "    WHERE pr_item_sk IS NOT NULL\n",
    "    --this is GROUP BY 1 in original::same as pr_item_sk here::hive complains anyhow\n",
    "    GROUP BY pr_item_sk\n",
    "  ) p\n",
    "  INNER JOIN (\n",
    "    SELECT\n",
    "      ws_item_sk,\n",
    "      SUM(ws_net_paid) AS revenue\n",
    "    FROM web_sales ws\n",
    "    -- Select date range of interest\n",
    "    LEFT SEMI JOIN (\n",
    "      SELECT d_date_sk\n",
    "      FROM date_dim d\n",
    "      WHERE d.d_date >= '${q11_startDate}'\n",
    "      AND   d.d_date <= '${q11_endDate}'\n",
    "    ) dd ON ( ws.ws_sold_date_sk = dd.d_date_sk )\n",
    "    WHERE ws_item_sk IS NOT null\n",
    "    --this is GROUP BY 1 in original::same as ws_item_sk here::hive complains anyhow\n",
    "    GROUP BY ws_item_sk\n",
    "  ) s\n",
    "  ON p.pr_item_sk = s.ws_item_sk\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd3578-e464-43ed-bbe8-ce8356c45924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "# !cat queries/q26/results/q26-result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341713e-b28b-42d3-bb58-2fc5221832a5",
   "metadata": {},
   "source": [
    "## Chevney "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d68c14-0771-4136-a061-f41a24f2bf24",
   "metadata": {},
   "source": [
    "## 7) Query 06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b94bca-0ecd-45e1-9c45-a37d1b9f747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/28 22:25:34 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: TPCx-BB-dataset/TEMP_TABLE1.ptxt.\n",
      "java.io.FileNotFoundException: File TPCx-BB-dataset/TEMP_TABLE1.ptxt does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/janiswaser/Desktop/Data_Management/Homework7/TPCx-BB-dataset/TEMP_TABLE1.ptxt. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m q06_LIMIT=\u001b[32m100\u001b[39m\n\u001b[32m      2\u001b[39m q06_YEAR=\u001b[32m2001\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m store = \u001b[43mget_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTEMP_TABLE1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m web = get_table(\u001b[33m'\u001b[39m\u001b[33mTEMP_TABLE2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m c = get_table(\u001b[33m'\u001b[39m\u001b[33mcustomer\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(table)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load table from TPCxx-BB dataset. Returning a dataframe read from parquet format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m get_table = \u001b[38;5;28;01mlambda\u001b[39;00m table: \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTPCx-BB-dataset/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.ptxt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/janiswaser/Desktop/Data_Management/Homework7/TPCx-BB-dataset/TEMP_TABLE1.ptxt. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "q06_LIMIT=100\n",
    "q06_YEAR=2001\n",
    "\n",
    "\n",
    "store = get_table('TEMP_TABLE1')\n",
    "web = get_table('TEMP_TABLE2')\n",
    "c = get_table('customer')\n",
    "\n",
    "join = (\n",
    "    web.join(\n",
    "    other = store, on = (store.customer_sk == web.customer_sk))\n",
    "    .join(other = c, on = (web.customer_sk == c.customer_sk))\n",
    ")\n",
    "\n",
    "filter = (\n",
    "    join.filter(\n",
    "        (col(\"web.second_year_total\") / col(\"web.first_year_total\")) > \n",
    "        (col(\"store.second_year_total\") / col(\"store.first_year_total\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "quary06_solution = filter.select(\n",
    "    (col(\"web.second_year_total\") / col(\"web.first_year_total\")).alias(\"web_sales_increase_ratio\"),\n",
    "    col(\"c.c_customer_sk\"),\n",
    "    col(\"c.c_first_name\"),\n",
    "    col(\"c.c_last_name\"),\n",
    "    col(\"c.c_preferred_cust_flag\"),\n",
    "    col(\"c.c_birth_country\"),\n",
    "    col(\"c.c_login\"),\n",
    "    col(\"c.c_email_address\")).orderBy(\n",
    "        desc(\"web_sales_increase_ratio\"),\n",
    "        \"c_customer_sk\",\n",
    "        \"c_first_name\",\n",
    "        \"c_last_name\",\n",
    "        \"c_preferred_cust_flag\",\n",
    "        \"c_birth_country\",\n",
    "        \"c_login\"\n",
    "    ).limit(q06_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c278e0-7f85-425f-a619-49e65227dee7",
   "metadata": {},
   "source": [
    "## Query 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b518c488-f6c0-4556-92ab-72e3eea53fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/28 22:27:59 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: TPCx-BB-dataset/TEMP_TABLE1.ptxt.\n",
      "java.io.FileNotFoundException: File TPCx-BB-dataset/TEMP_TABLE1.ptxt does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/janiswaser/Desktop/Data_Management/Homework7/TPCx-BB-dataset/TEMP_TABLE1.ptxt. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m q13_Year=\u001b[32m2001\u001b[39m\n\u001b[32m      2\u001b[39m q13_limit=\u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m store = \u001b[43mget_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTEMP_TABLE1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m web = get_table(\u001b[33m'\u001b[39m\u001b[33mTEMP_TABLE2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m c = get_table(\u001b[33m'\u001b[39m\u001b[33mcustomer\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(table)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load table from TPCxx-BB dataset. Returning a dataframe read from parquet format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m get_table = \u001b[38;5;28;01mlambda\u001b[39;00m table: \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTPCx-BB-dataset/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.ptxt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/janiswaser/Desktop/Data_Management/Homework7/TPCx-BB-dataset/TEMP_TABLE1.ptxt. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "q13_Year=2001\n",
    "q13_limit=100\n",
    "\n",
    "\n",
    "store = get_table('TEMP_TABLE1')\n",
    "web = get_table('TEMP_TABLE2')\n",
    "c = get_table('customer')\n",
    "\n",
    "join = (\n",
    "    web.join(\n",
    "    other = store, on = (store.customer_sk == web.customer_sk))\n",
    "    .join(other = c, on = (web.customer_sk == c.customer_sk))\n",
    ")\n",
    "\n",
    "filter = (\n",
    "    join.filter(\n",
    "        (col(\"web.second_year_total\") / col(\"web.first_year_total\")) > \n",
    "        (col(\"store.second_year_total\") / col(\"store.first_year_total\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "quary13_solution = filter.select(\n",
    "    (col(\"store.second_year_total\") / col(\"store.first_year_total\")).alias(\"storeSalesIncreaseRatio\"),\n",
    "    (col(\"web.second_year_total\") / col(\"web.first_year_total\")).alias(\"webSalesIncreaseRatio\"),\n",
    "    col(\"c_customer_sk\"),\n",
    "    col(\"c_first_name\"),\n",
    "    col(\"c_last_name\")\n",
    "    ).orderBy(\n",
    "        desc(\"webSalesIncreaseRatio\"),\n",
    "        \"c_customer_sk\",\n",
    "        \"c_first_name\",\n",
    "        \"c_last_name\",\n",
    "    ).limit(q13_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97250925-99e5-4263-82b1-d287bf7df79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
